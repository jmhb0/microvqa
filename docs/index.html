<!-- a comment -->
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="">
  <meta name="keywords" content="ViewNeTI, Viewpoint textual inversion, diffusion, 3D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4G6N4R7X3Q"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-4G6N4R7X3Q');
  </script>

  <!-- Math -->
  <script type="text/javascript" src="static/js/LaTeXMathML.js">
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <!-- <link rel="stylesheet" href="./static/css/bulma-carousel.min.css"> -->
  <!-- <link rel="stylesheet" href="./static/css/bulma-slider.min.css"> -->
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script> -->
  <!-- <script src="./static/js/bulma-slider.min.js"></script> -->
  <script src="./static/js/index.js"></script>
  <!-- Include Swiper.js CDN -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11/swiper-bundle.min.css" />
  <script src="https://cdn.jsdelivr.net/npm/swiper@11/swiper-bundle.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">



  <style>
  /* Make swiper1 wider */
  .mySwiper1 {
    width: 80% !important;
    max-width: 1200px !important; /* Increase from the default */
    margin: 0 auto 30px auto;
  }
  /* Make sure the images inside maintain proper aspect ratio */
  .mySwiper1 .swiper-slide img {
    width: 100%;
    height: auto;
    object-fit: contain;
  }
</style>

</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based
              Scientific Research</h1>
            <h3 class="title is-4">CVPR 2025</h3>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://jmhb0.github.io">James Burgess</a>*<sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/jeff-nirschl-56700918/">Jeffrey J Nirschl</a>*<sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://laubravo.github.io">Laura Bravo-SÃ¡nchez</a>*<sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/ale9806/">Alejandro Lozano</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/sanket-gupte-9b753b9a/">Sanket Rajan Gupte</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=f2XUw78AAAAJ&hl=en">Jesus G.
                  Galaz-Montoya</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://cs.stanford.edu/~yuhuiz/">Yuhui Zhang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://suyccc.github.io">Yuchang Su</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/disha-bhowmik/">Disha Bhowmik</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/zach-coman-4699a0178/">Zachary Coman</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/sarina-hasan-20734016b/">Sarina M. Hasan</a><sup>4</sup>,</span>
              <span class="author-block">
                <a href="https://lundberglab.stanford.edu">Alexandra Johannesson</a><sup>5</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/wleineweber/">William D. Leineweber</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/malvika-nair-814197209/">Malvika G Nair</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://cohenlaboratory.web.unc.edu/people/">Ridhi Yarlagadda</a><sup>3</sup>,</span>1
              <span class="author-block">
                <a href="https://www.linkedin.com/in/czuraski/">Connor Zuraski</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.wahchiulab.org/about">Wah Chiu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://cohenlaboratory.web.unc.edu">Sarah Cohen</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://bsky.app/profile/hansenjn.bsky.social">Jan
                  N. Hansen</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.czbiohub.org/leonetti/">Manuel D Leonetti</a><sup>6</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/chad-liu-14a77749/">Chad Liu</a><sup>6</sup>,</span>
              <span class="author-block">
                <a href="https://lundberglab.stanford.edu">Emma Lundberg</a><sup>1,5,6</sup>,</span>
              <span class="author-block">
                <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a><sup>1,6</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Stanford University,</span>
              <span class="author-block"><sup>2</sup>Tsinghua University,</span>
              <span class="author-block"><sup>3</sup>University of North Carolina at Chapel Hill,</span>
              <span class="author-block"><sup>4</sup>Princeton University</span>
              <span class="author-block"><sup>5</sup>KTH Royal Institute of Technology</span>
              <span class="author-block"><sup>6</sup>Chan Zuckerberg Biohub Network</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.13399" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/jmhb/microvqa"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <!-- <i class="fab fa-github"></i> -->
                      <i class="fa-huggingface"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <!-- Code Link -->
                <span class="link-block">
                  <a href="https://github.com/jmhb0/microvqa" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Leaderboard Link. -->
                <span class="link-block">
                  <a href="#testing-mllms" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <!-- <i class="fab fa-github"></i> -->
                      <i class="fa-solid fa-trophy"></i>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          <p class="mt-4">Two key themes in AI research are reasoning in large language models (LLMs) and
            applying LLMs to scientific research. Our work bridges both by introducing <span
              class="has-text-info"><b>MicroVQA</b>, a benchmark that evaluates LLM reasoning on multiple-choice
             questions about microscopy images, created by expert biologists.</span></p>
        </h2>
        <div class="has-text-centered">
          <img src="./static/images/pull.png" style="width: 90%; max-width: 800px; height: auto;"
            alt="MicroVQA pull figure." />
        </div>
      </div>
      <div class="content">
        <h3>Contributions:</h3>
        <ul>
          <li><i class="fas fa-flask"></i> Manually created by practicing biologists, these questions reflect tasks where LLMs
            could meaningfully assist biological research, and each requires multimodal reasoning.</li>
          <li><i class="fas fa-robot"></i> We also advance the practice of AI benchmarking with our RefineBot method,
            which makes multiple-choice questions more challenging by using LLM feedback to remove language shortcuts.
          </li>
        </ul>
      </div>
    </div>
  </section>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-4">Key Elements of MicroVQA</h2>

        <div class="content">
          <ul>
            <li><strong>Useful for real scientific researchers:</strong> MicroVQA has 1042 questions from
              three tasks that are important to scientific research. The tasks are: expert visual understanding,
              hypothesis generation, and experimental proposal.</li>
            <li><strong>Difficult reasoning:</strong> The questions require domain knowledge and multistep reasoning at the research level of difficulty. Existing vision-language benchmarks for science fall short on one of these dimensions: for example, <a href="https://mmmu-benchmark.github.io/">MMMU-Pro</a> has complex reasoning but at the college-level (from exams), while <a href="https://arxiv.org/abs/2402.09181">OmnimedVQA</a> is graduate-level but has less complex reasoning.</li>
            <li><strong>High-quality, human-created questions:</strong> The questions were created and verified by 11
              experts in diverse biological research disciplines. Each question took on average 30mins to write.</li>
            <li><strong>Challenging for current LLMs:</strong> We tested state-of-the-art MLLMs and specialized medical
              models on MicroVQA, with the top model scoring only 52.8% (<a href="#testing-mllms">leaderboard</a>).</li>
            <li><strong>Multimodal:</strong> There are text-only benchmarks that are both research-level and require difficult reasoning â  <a href="https://arxiv.org/abs/2311.12022">GPQA</a> and (most of) <a href="https://arxiv.org/abs/2407.10362">LAB-Bench</a>. But scientific research is multimodal, so we make questions that need image understanding. To make sure that MicroVQA is vision-centric, we intoroduce the <a href="#refinebot"> RefineBot</a> method to remove language shortcuts from multiple-choice questions.
            </li>
          </ul>
        </div>
        Explore the benchmark here:
        <iframe src="https://huggingface.co/datasets/jmhb/microvqa/embed/viewer/default/dev" frameborder="0"
          width="100%" style="height: 700px !important;"></iframe>
      </div>
  </section>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-4">Building MicroVQA</h2>

        <div class="content">
          <p><strong class="has-text-info">Why build MicroVQA?</strong>
            Scientific research involves reasoning about complex data while integrating domain-specific knowledge. A
            lot of recent excitement around LLMs comes from the hope that they can augment humans, either as chat
            assistants [<a href="https://www.nature.com/articles/s41586-024-07618-3">1</a>, <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/36850592258c8c41cecdaa3dea5ff7de-Abstract-Datasets_and_Benchmarks_Track.html">2</a>, <a href="https://www.biorxiv.org/content/10.1101/2024.04.30.591835.abstract">3</a>, <a href="https://arxiv.org/abs/2306.10070">4</a>] or as agents [<a href="https://www.nature.com/articles/s41586-023-06792-0">1</a>,<a href="https://www.nature.com/articles/s42256-024-00832-8">2</a>,<a href="https://www.nature.com/articles/s41592-023-01912-0">3</a>, <a href="https://arxiv.org/abs/2502.18864">4</a>]. There are some compelling visions for how these systems might look [<a href="https://arxiv.org/abs/2205.02007">1</a>,<a href="https://arxiv.org/abs/2404.02831">2</a>] but there's now a big need to define concrete tasks that would be useful for real scientists - we need realistic and challenging benchmarks.
          </p>

          <p><strong class="has-text-info">Defining the tasks.</strong> Step 1 was to define tasks, or categories of
            questions to guide benchmark collection. We developed them with 8 collaborators over ~16h of discussions. The tasks are:
          </p>

          <ul>
            <li><strong>Expert visual understanding</strong>, which commonly involves anomaly detection or image comparisons. Analysis
            must consider the sample preparation context and expert knowledge is needed to evaluate biological
            features and technical artifacts.
          </li>
          <li><strong>Hypothesis generation</strong> considers the experimental context and tries to find an explanation for
            what we see in the image. It requires abductive reasoning, since you need to select from many possible
            hypotheses given incomplete information.
            </li>
            <li><strong>Experimental proposal</strong> requires deciding on next steps to validate a given hypothesis.
            It requires knowledge about experimental protocols and reasoning about whether the experiment would prove
            that hypothesis.</li>
            </ul>

          <!-- Type Gallery -->
          <div class="swiper mySwiper1">
            <div class="swiper-wrapper">
              <div class="swiper-slide">
                <img src="static/images/example-question-20.png" alt="task 1 example">
                <!-- <div class="caption">This is the first image</div> -->
              </div>
              <div class="swiper-slide">
                <img src="static/images/example-question-137.png" alt="task 2 example">
                <!-- <div class="caption">Another caption for image 2</div> -->
              </div>
              <div class="swiper-slide">
                <img src="static/images/example-question-272.png" alt="task 3 example">
                <!-- <div class="caption">Image 3 has a different caption</div> -->
              </div>
            </div>
            <div class="swiper-button-next"></div>
            <div class="swiper-button-prev"></div>
            <div class="swiper-pagination"></div>
          </div>
          <!-- <div class="slideshow-container" id="slideshow1">
                <img class="slide active" src="./static/images/example1.png" alt="Expert visual understanding example">
                <div class="caption"><em>Expert Visual Understanding Example</em></div>
                <img class="slide" src="./static/images/example2.png" alt="Hypothesis generation example">
                <div class="caption"><em>Hypothesis Generation Example</em></div>
                <img class="slide" src="./static/images/example3.png" alt="Experimental proposal example">
                <div class="caption"><em>Experimental Proposal Example</em></div>
                <button class="prev" onclick="changeSlide(-1, 'slideshow1')">&#10094;</button>
                <button class="next" onclick="changeSlide(1, 'slideshow1')">&#10095;</button>
            </div> -->
        </div>
      </div>
    </div>
  </section>

  <section class="hero" id="refinebot">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-4">RefineBot: Making MCQs That Test Reasoning and Remove Language Shortcuts</h2>

        <div class="content">
          <p>When building QA & VQA evaluations, you typically start with a question string and answer string written by a person. Then it's common to use an LLM to transform it into a good multiple-choice question (MCQ), including the wrong answers that are called 'distractors'. Here we'll use a very easy biology question to demonstrate.</p>

          <figure class="image">
            <img src="./static/images/bot1.png" style="width: 100%; max-width: 800px; height: auto;"
              alt="Example question generation">
          </figure>

          <p>We found that this naive approach leads to questions that are very easy for LLMs to solve - GPT-4o could
            get 90% of questions correct, even without the image. We were confident that the original questions really do need the
            image, so we concluded that the question generation process was introducing language shortcuts, meaning LLMs could
            cheat and solve them using test-taking strategies. We identified 3 types of shortcuts:</p>

          <figure class="image">
            <img src="./static/images/bot2.png" style="width: 100%; max-width: 800px; height: auto;"
              alt="Three types of shortcuts">
          </figure>

          <p><strong>Shortcut 1 - visual giveaway.</strong> The text 'gives away' the image content so it's trivial to answer the
            question.</p>
          <p><strong>Shortcut 2 - weak distractors.</strong> The LLM generates implausible or weak distractors that are easy to eliminate.</p>
          <p><strong>Shortcut 3 - language bias.</strong> The model can make an educated guess based on the prior learned by the LLM [<a href="https://arxiv.org/abs/1612.00837">1</a>].</p>

          <p>We created RefineBot to fix this (<a href="https://github.com/jmhb0/microvqa">code here</a>). The key
            idea is that if you give the question to the LLM without the image (box 1 below), then the LLM's
            chain-of-thought response can show you what language shortcut it used (box 2). Then you can simply prompt
            the LLM to rewrite the question in a way that removes the shortcut (boxes 3 and 4).</p>

          <figure class="image">
            <img src="./static/images/bot3.png" style="width: 100%; max-width: 800px; height: auto;"
              alt="RefineBot process">
          </figure>

          <p>The final RefineBot system applies this basic idea in a loop, with an extra check that the revised
            question matches the original question.</p>

          <figure class="image">
            <img src="./static/images/refinebot.png" style="width: 40%; max-width: 800px; height: auto;"
              alt="Final RefineBot system">
          </figure>

          <p>While we designed this to deal with language shortcuts in particular, we think that a similar strategy
            could make any multiple-choice question harder.</p>
        </div>

      </div>
    </div>
  </section>

  <section class="hero" id="testing-mllms">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-4">Testing MLLMs on MicroVQA</h2>
        <div class="content">
          <p>We tested MicroVQA on current frontier models:</p>

          <iframe src="static/leaderboard/leaderboard.html" width="100%" height="500px" style="border: none;"></iframe>

          <ul class="content">
            <li>MicroVQA is too challenging for all existing multimodal LLMs, scoring below 53%.</li>
            <li>There is little gap between the top closed- and open-source models.</li>
            <li>For a given model family, there is very little difference between the larger model like Qwen2-VL-72B
              and its smaller counterpart Qwen2-VL-7B. Note that the smaller models often have the same vision
              encoder.</li>
            <li>Standard fine-tuning methods for medical fine-tuning (as in Llava-Med compared with LLaVA-Mistral-7B) do improve performance a bit.
            </li>
            <li>The human baseline was 51% - this reflects that biology is very specialized, so even expert humans
              will get many questions wrong.</li>
            <li>Starred models (*) were used by the RefineBot in MCQ - GPT-4o and Claude-3.5-Sonnet. This could induce
              a small bias that makes their final performance relatively worse.</li>
            <li>We perform 'no-image' and 'choices-only' ablations in the paper.</li>
          </ul>
        </div>
      </div>
    </div>
  </section>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-4">What do MLLMs get wrong?</h2>

        <div class="content">
          <p><strong class="has-text-info">Understanding error modes.</strong>
            We manually reviewed 30 random samples of errors by Claude-3.5-Sonnet. We spent about 45 minutes per sample to properly understand the failures. Here's some example:
          </p>

          <!-- Gallery for Error Examples -->
          <div class="swiper mySwiper2">
            <div class="swiper-wrapper">
              <div class="swiper-slide">
                <img src="./static/images/767_157_770_anat_blooms-4_task-1_incorrect.png"
                  alt="Example perception error">
              </div>
              <div class="swiper-slide">
                <img src="./static/images/806_165_809_neur_blooms-3_task-1_incorrect.png"
                  alt="Example misconception (knowledge) error">
              </div>
              <div class="swiper-slide">
                <img src="./static/images/771_158_774_path_blooms-2_task-2_incorrect.png"
                  alt="Example overgeneralization error">
              </div>
            </div>
            <div class="swiper-button-next"></div>
            <div class="swiper-button-prev"></div>
            <div class="swiper-pagination"></div>
          </div>
          <!-- <div class="slideshow-container" id="slideshow2">
                <img class="slide active" src="./static/images/767_157_770_anat_blooms-4_task-1_incorrect.png" alt="Example perception error">
                <p class="has-text-centered"><em>Example perception error</em></p>
                <img class="slide" src="./static/images/806_165_809_neur_blooms-3_task-1_incorrect.png" alt="Example misconception (knowledge) error">
                <p class="has-text-centered"><em>Example misconception (knowledge) error</em></p>
                <img class="slide" src="./static/images/771_158_774_path_blooms-2_task-2_incorrect.png" alt="Example overgeneralization error">
                <p class="has-text-centered"><em>Example overgeneralization error</em></p>
                <button class="prev" onclick="changeSlide(-1, 'slideshow2')">&#10094;</button>
                <button class="next" onclick="changeSlide(1, 'slideshow2')">&#10095;</button>
              </div> -->

          <ul class="content">
            <li><strong>50%</strong> were <strong>perception errors</strong> - failing to interpret the image. Many
              responses tend to rely on the 'language bias' towards common image content (that we discuss above).</li>
            <li><strong>30%</strong> were <strong>misconception (knowledge) errors</strong> about nuanced
              biomedical knowledge.</li>
            <li><strong>13%</strong> were <strong>over-generalization or over-simplification</strong> - the model
              answers a less-specific and less-nuanced version of the actual question.</li>
            <li><strong>7%</strong> were <strong>hallucinations</strong> about the question text added in the
              chain-of-thought response.</li>
          </ul>
          <p>
            This suggests that the most important next steps for stronger microscopy MLLMs are: (1) to improve microscopy-specific image
            understanding in MLLMs, and (2) to improve knowledge, possibly with some RAG-based methods.
          </p>
        </div>
      </div>
    </div>
  </section>


  <section class="hero" , id="blooms">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-4">Bloom's Taxonomy to Measure Reasoning Difficulty</h2>

        <div class="content">
          <!-- <p><strong class="has-text-info">Why use Bloom's Taxonomy?</strong>
            Bloom's Taxonomy classifies cognitive skills into six levels, from basic recall to complex reasoning.
            While multiple-choice questions (MCQs) cannot assess the highest levelâcreationâthey effectively test
            comprehension, application, analysis, and evaluation <a href="#">[1]</a><a href="#">[2]</a><a
              href="#">[3]</a>.
            MicroVQA applies Bloom's Taxonomy to systematically compare the reasoning demands of multimodal biomedical
            benchmarks.
          </p> -->

          <p><strong class="has-text-info">MicroVQA's focus on high reasoning levels.</strong>
            We argued that prior MLLM benchmarks for science tend to emphasize recall and basic comprehension, probably 
            because they are derived from educational exams and text books. Our goal was to test more challenging scientific 
            reasoning by recruiting experts to create questions that reflect real research tasks, like analyzing microscopy 
            images in novel experimental contexts, and evaluating hypotheses. 
          </p>
          <p>
            <p><strong class="has-text-info">Bloom's taxonomy.</strong>
            To quantify this more challenging reasoning, we used the Bloom's taxonomy [<a href="https://en.wikipedia.org/wiki/Bloom%27s_taxonomy">1</a>], which classifies cognitive skills into six levels, from basic recall to complex reasoning. While multiple-choice questions (MCQs) cannot assess the highest level â called 'creation' â they effectively test comprehension, application, analysis, and evaluation [<a href="https://www.lifescied.org/doi/10.1187/cbe.08-05-0024">1</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/28231408/">2</a>, <a href="https://books.google.com/books/about/A_Taxonomy_for_Learning_Teaching_and_Ass.html?id=EMQlAQAAIAAJ">3</a>]. 
            We trained an LLM to classify MCQs into Bloom's levels. This lets us compare reasoning levels between different vision-language MCQ benchmarks in science:
          </p>

          <figure class="image">
            <img src="./static/images/blooms.png" style="width: 60%; max-width: 800px; height: auto;"
              alt="Bloom's Taxonomy in MicroVQA">
            <figcaption style="text-align: center; font-size: 14px; color: #555; margin-top: 8px;">
              Composition of scientific MLLM benchmarks regarding estimated Bloom's taxonomy.
            </figcaption>
          </figure>
          <p>Among research-level benchmarks, MicroVQA has the highest-level reasoning. 
            The next highest are <a href="https://arxiv.org/abs/2407.10362">LAB-Bench</a> at the research level, and MMMU-pro at the college level. 
            Another finding is that among benchmarks with higher level reasoning, data scale becomes very challenging - harder questions are harder to collect. Specifically, MicroVQA has 1,042 samples, LAB-Bench's vision-language subset has only 181, and MMMU-Pro has 1,730.
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Citation</h2>
    <pre><code>@article{
  }</code></pre>
  </div>
  
</section>



  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              The template for this page is taken from <a href="https://nerfies.github.io/">Nerfies</a>. If you reuse
              their <a href="https://github.com/nerfies/nerfies.github.io">code</a>, please link to their site.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Slider info -->
  <script>
    var swiper1 = new Swiper(".mySwiper1", {
      loop: true,
      pagination: {
        el: ".mySwiper1 .swiper-pagination",
        clickable: true,
      },
      navigation: {
        nextEl: ".mySwiper1 .swiper-button-next",
        prevEl: ".mySwiper1 .swiper-button-prev",
      },
      autoplay: {
        delay: 3000,
        disableOnInteraction: true,
      },
    });

    var swiper2 = new Swiper(".mySwiper2", {
      loop: true,
      pagination: {
        el: ".mySwiper2 .swiper-pagination",
        clickable: true,
      },
      navigation: {
        nextEl: ".mySwiper2 .swiper-button-next",
        prevEl: ".mySwiper2 .swiper-button-prev",
      },
      autoplay: {
        delay: 4000, // Different delay if needed
        disableOnInteraction: true,
      },
    });
  </script>


</body>

</html>