<!-- a comment -->
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="ViewNeTI, Viewpoint textual inversion, diffusion, 3D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4G6N4R7X3Q"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-4G6N4R7X3Q');
</script> 

  <!-- Math -->
  <script type="text/javascript"
  src="static/js/LaTeXMathML.js">
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <!-- <link rel="stylesheet" href="./static/css/bulma-carousel.min.css"> -->
  <!-- <link rel="stylesheet" href="./static/css/bulma-slider.min.css"> -->
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script> -->
  <!-- <script src="./static/js/bulma-slider.min.js"></script> -->
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</h1>
          <h3 class="title is-4">Preprint</h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">James Burgess</a>*<sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Jeffrey J Nirschl</a>*<sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Laura Bravo-Sánchez</a>*<sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Alejandro Lozano</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Sanket Rajan Gupte</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Jesus G. Galaz-Montoya</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Yuhui Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Yuchang Su</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="">Disha Bhowmik</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="">Zachary Coman</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="">Sarina M. Hasan</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="">Alexandra Johannesson</a><sup>5</sup>,</span>
            <span class="author-block">
              <a href="">William D. Leineweber</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Malvika G Nair</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="">Ridhi Yarlagadda</a><sup>3</sup>,</span>1
            <span class="author-block">
              <a href="">Connor Zuraski</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Wah Chiu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Sarah Cohen</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="">Jan N. Hansen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Manuel D Leonetti</a><sup>6</sup>,</span>
            <span class="author-block">
              <a href="">Chad Liu</a><sup>6</sup>,</span>
            <span class="author-block">
              <a href="">Emma Lundberg</a><sup>6</sup>,</span>
            <span class="author-block">
              <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Stanford University,</span>
            <span class="author-block"><sup>2</sup>Tsinghua University,</span>
            <span class="author-block"><sup>3</sup>University of North Carolina at Chapel Hill,</span>
            <span class="author-block"><sup>4</sup>Princeton University</span>
            <span class="author-block"><sup>5</sup>KTH Royal Institute of Technology</span>
            <span class="author-block"><sup>6</sup>Chan Zuckerberg Biohub Network</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://jmhb0.github.io/assets/microvqa.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="ai ai-arxiv"></i> -->
                  </span>
                  <span>pdf</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/microvqa/microvqa"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="fab fa-github"></i> -->
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
       An release an expert-curated benchmark for research-level reasoning in biological microscopy. We also propose a novel method for making multiple-choice visual-question answering (VQA) more challenging.
      </h2>
      <img src="./static/images/pull.png"
                 class=""
                 alt="MicroVQA pull figure."/>
      <!-- <p>The idea: to generate a view of a 3D object at camera pose $\mathbf{R}_i$, find a point in text embedding space - the '3D view token' - to condition the diffusion generation. To generate different views $\mathbf{R}_1$, $\mathbf{R}_2$, ... learn a function for predicting the word token.</p> -->
    </div>
  </div>
</section>


<!--/ Abstract. -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Scientific research demands sophisticated reasoning over multimodal data, a challenge especially prevalent in biology. Despite recent advances in multimodal large language models (MLLMs) for AI-assisted research, existing multimodal reasoning benchmarks target up to college-level difficulty, while research-level benchmarks emphasize lower-level perception, falling short of the complex multimodal reasoning needed for scientific discovery. To bridge this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark designed to assess three reasoning capabilities vital in research workflows: expert image understanding, hypothesis generation, and experiment proposal. MicroVQA consists of 1,061 multiple-choice questions (MCQs) curated by biological experts across diverse microscopy modalities, ensuring VQA samples represent real scientific practice. We find that standard MCQ creation methods do not properly test our targeted reasoning capabilities, motivating a new two-stage pipeline: an optimized LLM prompt structures question-answer pairs into MCQs; then, an agent-based 'RefineBot' generates more challenging distractors. Benchmarking on state-of-the-art MLLMs reveal a peak performance of 43%; models with smaller LLMs only slightly underperform top models, suggesting that language-based reasoning is less challenging than multimodal reasoning; and tuning with scientific articles enhances performance. Expert analysis of chain-of-thought reasoning failures indicates that multimodal reasoning errors are frequent, followed by knowledge errors and overgeneralization. These insights highlight the challenges in multimodal scientific reasoning, showing MicroVQA is a valuable resource advancing AI-driven biomedical research. MicroVQA is available <a href=
            "https://huggingface.co/datasets/microvqa/microvqa">here</a>.
          </p>
        </div>
      </div>
    </div>



<!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Two key findings about 3D control in the text input space</h2>
    <p><b>Finding 1</b> (figure below, left): the text input space has a <i>continuous view-control manifold</i>. </p>
    </p>Evidence: using just 6 views of one scene, we learn to generate 3D view tokens that interpolate to novel views.</p>
    <br>
    <p><b>Finding 2</b> (figure below, right): the text input space likely has a <i>semantically disentangled view control manifold</i>, meaning the same 3D view token can generalize to many scenes. 
      <p>Evidence: we learn 3D view tokens across 88 scenes in DTU, and it generalizes to new scenes. Specifically, we use it to do novel view synthesis from a single image.</p>
    <br>
    <div class="content">
      <img src="./static/images/findings.png" class="" alt="ViewNeTI key findings."/>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Our approach: learn a small network to predict 3D view tokens</h2>
    <p>We learn a small neural mapping network that takes camera parameters and predicts a `3D view token’. We then add the 3D view token to a text prompt to generate the image in that viewpoint. The train the network with <a href="https://textual-inversion.github.io/">Textual inversion</a>.</p>
    <br>
    <p>In the figure below, the camera parameters are a vector, $R_i$, and we also condition on the diffusion timestep, $t$, and the UNet layer $\ell$. This is for "finding 1", and for "finding 2", we also predict a token for each object (see the paper).</p>
    <br>
    <div class="content">
      <img src="./static/images/system-fig.png" class="" alt="system figure."/>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Conclusion: 2D models have 3D representations in their text space</h2>
    <p>Since we kept the text-to-image diffusion model frozen, and we only learned a simple network, we conclude that text-to-image diffusion models likely have internal 3D scene representations. This might help explain why models like Stable Diffusion generate such compelling 3D features, like in this image below where infilling the background creates shadows that are consistent with the original object. A few other works study 3D representations from different perspectives - see Related Work below </p>
  </div>
  <br>
  <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/infilling_.png" alt="Infilling StableDiffusion example"/>
          </div>

</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Applications</h2>
    <b>1. View-controlled text-to-image generation. </b>
    <p>Add the 3D view token to a new text prompt to control the 3D viewpoint for new objects (sample results below, left).</p>
    <br>
    <b>2. Novel view synthesis from 1 image (or more images), with very good sample efficiency</b>
    <p>By pretraining on 88 scenes from DTU or on 50 scenes from Objaverse, we can do single-image NVS (sample results below, right). Because we’re working through a pretrained diffusion model, the generated views have excellent photorealism and LPIPS. We also find that feed-forward methods like Zero-1-to-3 can’t learn any camera control on 50 scenes, and need 800k pretraining scenes to get good results (paper supplementary). 
  </div>
  <br>
  <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/applications.png" alt="Infilling StableDiffusion example"/>
          </div>

</section>



<!-- Related Work. -->
<!--
<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Related work</h2>
    <div class="content has-text-justified">
      <p>
        <b>3D representations in 2D models</b> have been investigated by a few works that probe Unet activations over many diffusion timesteps. <a href="https://arxiv.org/abs/2310.06836">Zhan et al.</a> trains linear SVMs to predict 3D relations between two regions in the same image. <a href="https://arxiv.org/abs/2306.05720">Chen et al.</a> train linear probes for depth and salient object / background segmentation, and also perform latent intervention during generation to alter geometric properties. <a href="https://arxiv.org/abs/2404.08636">El Banani et al.</a> train a convolution network for estimating depth and surface normals, while also using latents for 2-image visual correspondence in zero-shot. By contrast, ViewNeTI (our work) studies 3D control in the word embedding space by learning `3D view tokens’. 
      </p>

      <br>
      <p>
        In <b>view-controlled text-to-image generation</b>, a few concurrent works had some similar ideas. In <a href="https://ttchengab.github.io/continuous_3d_words/">Continuous 3D words</a>, they control viewpoint as well as lighting and pose in the word embedding space, but also fine-tune Loras. Later, <a href="https://customdiffusion360.github.io/">Kumari et al.</a> adds camera control to model customization methods. 
      </p>

      <br>
      <p>
        In <b>novel view synthesis</b>, the closest is <a href="https://sites.google.com/view/dreamsparse-webpage">DreamSparse</a>, because they are interested in leveraging a pretrained diffusion models and learning few parameters for data efficiency, although they require at least 2 input views. 
      </p>

      <br>
      <p>
      In terms of <b>methods</b>, our work uses <a href="https://textual-inversion.github.io/">textual inversion</a> that was developed for diffusion model personalization, but we adapt it to 3D novel view synthesis. We use ideas and code from the recent <a href="https://neuraltextualinversion.github.io/NeTI/">Neural Textual Inversion (NeTI)</a> model.
      </p>
    </div>
  </div>
</div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4">In hindsight ...</h2>
    <p>
      At the start of the project, we were mainly excited by the possibility that image diffusion models had learned an internal 3D model from 2D supervision. This was based on qualitative results like the car infilling we show above, which is the paper Fig.1. This seems to have been a good intuition, since other works like <a href="https://arxiv.org/abs/2310.06836">Zhan et al.</a> showed a very similar figure as their motivation, and we later saw similar figures used in various talks. It’s also stated as a motivation in <a href="https://zero123.cs.columbia.edu/">Zero-1-to-3</a>. </p>

      <br>
      <p>Despite the motivation being understanding 3D representations, our earlier drafts emphasized the application to novel view synthesis (NVS). This seemed like the highest impact contribution: ViewNeTI is very sample efficient (learning to do single-image NVS from as few as 50 scenes); and it seemed very well-motivated to do NVS via a frozen diffusion model, since it enables you to leverage knowledge from the massive 2D pre-training datasets. The key advantage is that it reduces the reliance on large 3d datasets for multi view training, however we underestimated how willing the 3D vision community was to create and work with large 3D datasets, and overestimated their excitement for sample efficient methods (especially since our results have the best LPIPS but not the best PSNR compared to other methods that use NeRFs). On the other hand, there was a lot more interest in understanding 3D representations than we expected, which was the focus of our final paper. </p>

      <br>
      <p>We only identified view-controlled text-to-image generation as an application much later, and we were surprised to find that this is a very promising direction, with a few related works by <a href="https://ttchengab.github.io/continuous_3d_words/">Cheng et al.</a> and <a href="https://arxiv.org/abs/2404.12333">Kumari et al</a>.  
      </p>

    </div>

</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{burgess2025viewpoint,
  title={Viewpoint Textual Inversion: Discovering Scene Representations and 3D View Control in 2D Diffusion Models},
  author={Burgess, James and Wang, Kuan-Chieh and Yeung-Levy, Serena},
  booktitle={European Conference on Computer Vision},
  pages={416--435},
  year={2025},
  organization={Springer}
}</code></pre>
  </div>
</section> -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
          The template for this page is taken from <a href="https://nerfies.github.io/">Nerfies</a>.  If you reuse their <a href="https://github.com/nerfies/nerfies.github.io">code</a>, please link to their site.
          </p>
          <p>
          .
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
