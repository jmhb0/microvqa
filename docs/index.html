<!-- a comment -->
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="">
  <meta name="keywords" content="ViewNeTI, Viewpoint textual inversion, diffusion, 3D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4G6N4R7X3Q"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-4G6N4R7X3Q');
  </script>

  <!-- Math -->
  <script type="text/javascript" src="static/js/LaTeXMathML.js">
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <!-- <link rel="stylesheet" href="./static/css/bulma-carousel.min.css"> -->
  <!-- <link rel="stylesheet" href="./static/css/bulma-slider.min.css"> -->
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script> -->
  <!-- <script src="./static/js/bulma-slider.min.js"></script> -->
  <script src="./static/js/index.js"></script>
  <!-- Include Swiper.js CDN -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11/swiper-bundle.min.css" />
  <script src="https://cdn.jsdelivr.net/npm/swiper@11/swiper-bundle.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based
              Scientific Research</h1>
            <h3 class="title is-4">CVPR 2025</h3>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://jmhb0.github.io">James Burgess</a>*<sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/jeff-nirschl-56700918/">Jeffrey J Nirschl</a>*<sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://laubravo.github.io">Laura Bravo-Sánchez</a>*<sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/ale9806/">Alejandro Lozano</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/sanket-gupte-9b753b9a/">Sanket Rajan Gupte</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=f2XUw78AAAAJ&hl=en">Jesus G.
                  Galaz-Montoya</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://cs.stanford.edu/~yuhuiz/">Yuhui Zhang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://suyccc.github.io">Yuchang Su</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/disha-bhowmik/">Disha Bhowmik</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/zach-coman-4699a0178/">Zachary Coman</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/sarina-hasan-20734016b/">Sarina M. Hasan</a><sup>4</sup>,</span>
              <span class="author-block">
                <a href="https://lundberglab.stanford.edu">Alexandra Johannesson</a><sup>5</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/wleineweber/">William D. Leineweber</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/malvika-nair-814197209/">Malvika G Nair</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://cohenlaboratory.web.unc.edu/people/">Ridhi Yarlagadda</a><sup>3</sup>,</span>1
              <span class="author-block">
                <a href="https://www.linkedin.com/in/czuraski/">Connor Zuraski</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.wahchiulab.org/about">Wah Chiu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://cohenlaboratory.web.unc.edu">Sarah Cohen</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.de/citations?hl=sk&user=ONpdBm8AAAAJ&view_op=list_works&sortby=pubdate">Jan
                  N. Hansen</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.czbiohub.org/sf/person/manuel-leonetti/">Manuel D Leonetti</a><sup>6</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/chad-liu-14a77749/">Chad Liu</a><sup>6</sup>,</span>
              <span class="author-block">
                <a href="https://lundberglab.stanford.edu">Emma Lundberg</a><sup>1,5,6</sup>,</span>
              <span class="author-block">
                <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a><sup>1,6</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Stanford University,</span>
              <span class="author-block"><sup>2</sup>Tsinghua University,</span>
              <span class="author-block"><sup>3</sup>University of North Carolina at Chapel Hill,</span>
              <span class="author-block"><sup>4</sup>Princeton University</span>
              <span class="author-block"><sup>5</sup>KTH Royal Institute of Technology</span>
              <span class="author-block"><sup>6</sup>Chan Zuckerberg Biohub Network</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/jmhb/microvqa"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <!-- <i class="fab fa-github"></i> -->
                      <i class="fa-huggingface"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <!-- Code Link -->
                <span class="link-block">
                  <a href="https://github.com/jmhb0/microvqa" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Leaderboard Link. -->
                <span class="link-block">
                  <a href="#testing-mllms" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <!-- <i class="fab fa-github"></i> -->
                      <i class="fa-solid fa-trophy"></i>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          <p class="mt-4">Two key themes in AI research are understanding reasoning in large language models (LLMs) and
            applying LLMs to scientific research. Our work bridges both by introducing <span
              class="has-text-info"><b>MicroVQA</b>, a benchmark that evaluates LLM reasoning on multiple-choice
              expert-curated questions about microscopy images.</span></p>
        </h2>
        <div class="has-text-centered">
          <img src="./static/images/pull.png" style="width: 90%; max-width: 800px; height: auto;"
            alt="MicroVQA pull figure." />
        </div>
      </div>
      <div class="content">
        <h3>Contributions:</h3>
        <ul>
          <li><i class="fas fa-flask"></i> Designed by practicing biologists, these questions reflect tasks where LLMs
            could meaningfully assist biological research, and each requires multimodal reasoning.</li>
          <li><i class="fas fa-robot"></i> We also advance the practice of AI benchmarking with our RefineBot method,
            which makes multiple-choice questions more challenging by using LLM feedback to remove language shortcuts.
          </li>
        </ul>
      </div>
    </div>
  </section>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-4">Key Elements of MicroVQA</h2>

        <div class="content">
          <ul>
            <li><strong>Useful for real scientific researchers:</strong> MicroVQA is composed of 1061 questions across
              three tasks designed to be central to scientific research - expert visual understanding, hypothesis
              generation, and experimental proposal.</li>
            <li><strong>Difficult reasoning:</strong> The questions require specialized research-level microscopy
              knowledge and high reasoning level (quantified in the Bloom's scale).</li>
            <li><strong>High-quality, human-created questions:</strong> The questions were created and verified by 11
              experts in diverse biological research disciplines.</li>
            <li><strong>Challenging for current LLMs:</strong> We tested state-of-the-art models spanning open-source,
              proprietary, and specialized medical models on MicroVQA, with the highest-scoring model achieving 44.2%
              accuracy over all tasks.</li>
            <li><strong>Multimodal:</strong> The multimodality of MicroVQA's questions goes beyond having an image
              attached to the question; the questions require understanding the image to solve them (e.g., localizing
              structures, comparing images, identifying anomalies). We introduce RefineBot, a method to reduce MLLM
              reliance on language.</li>
          </ul>
        </div>
        Explore the benchmark here:
        <iframe src="https://huggingface.co/datasets/jmhb/microvqa/embed/viewer/default/dev" frameborder="0"
          width="100%" style="height: 700px !important;"></iframe>
      </div>
  </section>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-4">Building MicroVQA</h2>

        <div class="content">
          <p><strong class="has-text-info">Why build MicroVQA?</strong>
            Scientific research involves reasoning about complex data while integrating domain-specific knowledge. A
            lot of recent excitement around LLMs comes from the hope that they can augment humans, either as chat
            assistants [1,2,3] or as agents [1,2,3]. There are many compelling visions for how these systems might
            look [1,2,3], but there's now a big need to define concrete tasks that would be useful for real scientists
            - we need realistic and challenging benchmarks.
          </p>

          <p><strong class="has-text-info">Defining the tasks.</strong> Step 1 was to define tasks, or categories of
            questions to guide benchmark collection. We developed them with 8 collaborators over ~15h of discussions.
          </p>

          <p><em>Expert visual understanding</em> commonly involves anomaly detection or image comparisons. Analysis
            must consider the sample preparation context and expert knowledge is needed to evaluate biological
            features and technical artifacts.</p>

          <p><em>Hypothesis generation</em> considers the experimental context and tries to find an explanation for
            what we see in the image. It requires abductive reasoning, since you need to select from many possible
            hypotheses given incomplete information.</p>

          <p><em>Experimental proposal</em> requires forming a hypothesis and deciding on next steps to validate it.
            It requires knowledge about experimental protocols and reasoning about whether the experiment would prove
            that hypothesis.</p>

          <!-- Type Gallery -->
          <div class="swiper mySwiper1">
            <div class="swiper-wrapper">
              <div class="swiper-slide">
                <img src="image1.jpg" alt="Image 1">
                <div class="caption">This is the first image</div>
              </div>
              <div class="swiper-slide">
                <img src="image2.jpg" alt="Image 2">
                <div class="caption">Another caption for image 2</div>
              </div>
              <div class="swiper-slide">
                <img src="image3.jpg" alt="Image 3">
                <div class="caption">Image 3 has a different caption</div>
              </div>
            </div>
            <div class="swiper-button-next"></div>
            <div class="swiper-button-prev"></div>
            <div class="swiper-pagination"></div>
          </div>
          <!-- <div class="slideshow-container" id="slideshow1">
                <img class="slide active" src="./static/images/example1.png" alt="Expert visual understanding example">
                <div class="caption"><em>Expert Visual Understanding Example</em></div>
                <img class="slide" src="./static/images/example2.png" alt="Hypothesis generation example">
                <div class="caption"><em>Hypothesis Generation Example</em></div>
                <img class="slide" src="./static/images/example3.png" alt="Experimental proposal example">
                <div class="caption"><em>Experimental Proposal Example</em></div>
                <button class="prev" onclick="changeSlide(-1, 'slideshow1')">&#10094;</button>
                <button class="next" onclick="changeSlide(1, 'slideshow1')">&#10095;</button>
            </div> -->
        </div>
      </div>
    </div>
  </section>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-4">RefineBot: Making MCQs That Test Reasoning and Remove Language Shortcuts</h2>

        <div class="content">
          <p>For evaluations, it's common to start with a 'question' and 'answer' and to use an LLM to generate a good
            multiple-choice question, including the wrong answers that are called 'distractors'. Here we'll use a very
            easy biology question to demonstrate.</p>

          <figure class="image">
            <img src="./static/images/bot1.png" style="width: 100%; max-width: 800px; height: auto;"
              alt="Example question generation">
          </figure>

          <p>We found that this naive approach leads to questions that are very easy for LLMs to solve - GPT-4o could
            get 90% of questions correct, even without the image. Since the original questions really do need the
            image, we concluded that the generated questions were introducing language shortcuts, meaning LLMs could
            cheat and solve them using test-taking strategies. We identified 3 types of shortcuts:</p>

          <figure class="image">
            <img src="./static/images/bot2.png" style="width: 100%; max-width: 800px; height: auto;"
              alt="Three types of shortcuts">
          </figure>

          <p><strong>Shortcut 1:</strong> The text 'gives away' the image content so it's trivial to answer the
            question.</p>
          <p><strong>Shortcut 2:</strong> The LLM generates implausible or weak distractors.</p>
          <p><strong>Shortcut 3:</strong> 'Language bias', is a known problem in VQA [1, 2].</p>

          <p>We created RefineBot to fix this (<a href="https://github.com/jmhb0/microvqa">code here</a>). The key
            idea is that if you give the question to the LLM without the image (box 1 below), then the LLM's
            chain-of-thought response can show you what language shortcut it used (box 2). Then you can simply prompt
            the LLM to rewrite the question in a way that removes the shortcut (boxes 3 and 4).</p>

          <figure class="image">
            <img src="./static/images/bot3.png" style="width: 100%; max-width: 800px; height: auto;"
              alt="RefineBot process">
          </figure>

          <p>The final RefineBot system applies this basic idea in a loop, with an extra check that the revised
            question matches the original question.</p>

          <figure class="image">
            <img src="./static/images/refinebot.png" style="width: 40%; max-width: 800px; height: auto;"
              alt="Final RefineBot system">
          </figure>

          <p>While we designed this to deal with language shortcuts in particular, we think that a similar strategy
            could make any multiple-choice question harder.</p>
        </div>

      </div>
    </div>
  </section>

  <section class="hero" id="testing-mllms">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-4">Testing MLLMs on MicroVQA</h2>
        <div class="content">
          <p>We tested MicroVQA on current frontier models:</p>

          <iframe src="static/leaderboard/leaderboard.html" width="100%" height="500px" style="border: none;"></iframe>

          <ul class="content">
            <li>MicroVQA is too challenging for all existing multimodal LLMs.</li>
            <li>There is little gap between frontier closed- and open-source models.</li>
            <li>For a given model family, there is very little difference between the larger model like Qwen2-VL-72B
              and its smaller counterpart Qwen2-VL-7B - note that the smaller models often have the same vision
              encoder.</li>
            <li>Standard fine-tuning methods for medical fine-tuning (as in Llava-Med) do improve performance a bit.
            </li>
            <li>The human baseline was 51% - this reflects that biology is very specialized, so even expert humans
              will get many questions wrong.</li>
            <li>Starred models (*) were used by the RefineBot in MCQ - GPT-4o and Claude-3.5-Sonnet. This could induce
              a small bias that makes their final performance relatively worse.</li>
          </ul>
        </div>
      </div>
    </div>
  </section>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-4">What do MLLMs get wrong?</h2>

        <div class="content">
          <p><strong class="has-text-info">Understanding error modes.</strong>
            We manually reviewed 30 random samples of errors by Claude-3.5-Sonnet, taking about 45 minutes to
            carefully understand the failures. Here's one example:
          </p>

          <!-- Gallery for Error Examples -->
          <div class="swiper mySwiper2">
            <div class="swiper-wrapper">
              <div class="swiper-slide">
                <img src="./static/images/767_157_770_anat_blooms-4_task-1_incorrect.png"
                  alt="Example perception error">
              </div>
              <div class="swiper-slide">
                <img src="./static/images/806_165_809_neur_blooms-3_task-1_incorrect.png"
                  alt="Example misconception (knowledge) error">
              </div>
              <div class="swiper-slide">
                <img src="./static/images/771_158_774_path_blooms-2_task-2_incorrect.png"
                  alt="Example overgeneralization error">
              </div>
            </div>
            <div class="swiper-button-next"></div>
            <div class="swiper-button-prev"></div>
            <div class="swiper-pagination"></div>
          </div>
          <!-- <div class="slideshow-container" id="slideshow2">
                <img class="slide active" src="./static/images/767_157_770_anat_blooms-4_task-1_incorrect.png" alt="Example perception error">
                <p class="has-text-centered"><em>Example perception error</em></p>
                <img class="slide" src="./static/images/806_165_809_neur_blooms-3_task-1_incorrect.png" alt="Example misconception (knowledge) error">
                <p class="has-text-centered"><em>Example misconception (knowledge) error</em></p>
                <img class="slide" src="./static/images/771_158_774_path_blooms-2_task-2_incorrect.png" alt="Example overgeneralization error">
                <p class="has-text-centered"><em>Example overgeneralization error</em></p>
                <button class="prev" onclick="changeSlide(-1, 'slideshow2')">&#10094;</button>
                <button class="next" onclick="changeSlide(1, 'slideshow2')">&#10095;</button>
              </div> -->

          <ul class="content">
            <li><strong>50%</strong> were <strong>perception errors</strong> - failing to interpret the image. Many
              responses tend to rely on the 'language bias' towards common image content (that we discuss above).</li>
            <li><strong>30%</strong> were <strong>misconception errors</strong> (or knowledge errors) about nuanced
              biomedical knowledge.</li>
            <li><strong>13%</strong> were <strong>over-generalization or over-simplification</strong> - the model
              answers a less-specific and less-nuanced version of the actual question.</li>
            <li><strong>7%</strong> were <strong>hallucinations</strong> about the question text added in the
              chain-of-thought response.</li>
          </ul>

          <!-- <p>
                  Finally, we conducted a <strong>language-only ablation</strong>, where GPT-4o was prompted without the image. This resulted in an accuracy of <strong>XX%</strong> of questions. However, this does not mean these questions are not vision-centric! We used an LLM to analyze the chain-of-thought responses (CoTs) of correct answers.
              </p>
  
              <p>
                  First, we found that <strong>YY%</strong> of answers were likely due to <strong>language bias</strong>: the correct answer was simply the most statistically likely choice. The remaining <strong>AA%</strong> of questions may truly not be vision-centric, which is an unavoidable challenge in VQA benchmarks (e.g., as shown by Cambrian Fig.3).
              </p> -->
        </div>
      </div>
    </div>
  </section>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-4">Bloom's Taxonomy to Measure Reasoning Difficulty</h2>

        <div class="content">
          <p><strong class="has-text-info">Why use Bloom's Taxonomy?</strong>
            Bloom's Taxonomy classifies cognitive skills into six levels, from basic recall to complex reasoning.
            While multiple-choice questions (MCQs) cannot assess the highest level—creation—they effectively test
            comprehension, application, analysis, and evaluation <a href="#">[1]</a><a href="#">[2]</a><a
              href="#">[3]</a>.
            MicroVQA applies Bloom's Taxonomy to systematically compare the reasoning demands of multimodal biomedical
            benchmarks.
          </p>

          <p><strong class="has-text-info">MicroVQA's focus on high reasoning levels.</strong>
            Prior benchmarks, often derived from educational exams and textbooks, emphasize recall and basic
            comprehension. In contrast, MicroVQA prioritizes higher-order reasoning by testing scientific research
            tasks such as analyzing microscopy images in novel experimental contexts and evaluating hypotheses.
            This structured approach ensures that MicroVQA better reflects the cognitive demands of real scientific
            reasoning, filling a crucial gap in multimodal AI evaluation.
          </p>

          <figure class="image">
            <img src="./static/images/blooms.png" style="width: 60%; max-width: 800px; height: auto;"
              alt="Bloom's Taxonomy in MicroVQA">
            <figcaption style="text-align: center; font-size: 14px; color: #555; margin-top: 8px;">
              Composition of scientific MLLM benchmarks regarding estimated Bloom's taxonomy.
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              The template for this page is taken from <a href="https://nerfies.github.io/">Nerfies</a>. If you reuse
              their <a href="https://github.com/nerfies/nerfies.github.io">code</a>, please link to their site.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Slider info -->
  <script>
    var swiper1 = new Swiper(".mySwiper1", {
      loop: true,
      pagination: {
        el: ".mySwiper1 .swiper-pagination",
        clickable: true,
      },
      navigation: {
        nextEl: ".mySwiper1 .swiper-button-next",
        prevEl: ".mySwiper1 .swiper-button-prev",
      },
      autoplay: {
        delay: 3000,
        disableOnInteraction: false,
      },
    });

    var swiper2 = new Swiper(".mySwiper2", {
      loop: true,
      pagination: {
        el: ".mySwiper2 .swiper-pagination",
        clickable: true,
      },
      navigation: {
        nextEl: ".mySwiper2 .swiper-button-next",
        prevEl: ".mySwiper2 .swiper-button-prev",
      },
      autoplay: {
        delay: 4000, // Different delay if needed
        disableOnInteraction: false,
      },
    });
  </script>


</body>

</html>